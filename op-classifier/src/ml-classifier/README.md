ml-classifier
=============

A system for inferring demographics for Twitter users. Currently two algorithms are supported, i) Support Vector Machine (SVM) as implemented in the libSVM package and the Gradient Boosted Decision Trees (GBDT) as implemented as the gbm package in R.

Prerequisite Packages:
---------------------
Apart from the standard enthought packages (such as numpy, logging, argparse etc.), you need to install the rpy2 package if you use GBDT (not needed for the SVM classifier).

The json format dataset files (training and test) are produced by the tfx program of the twitter-feature-extractor package (http://drgitlab.cs.mcgill.ca/demographic-inference/twitter-feature-extractor)

Installation:
-------------

If you have admin access to the machine, simply run the setup.py script.

  python setup.py install

If you are using a server where you do not have permission to install globally, you need to install the package locally for your user.

    python setup.py install --user

Sometimes, the locally installed binaries are placed into a directory that is not included in the $PATH variable. Therefore please make sure that the folder where the binaries are saved is in the system $PATH. Change your .bashrc and .bash_profile entries if needed.


Running Instruction:
-------------------

There are two executables.
    1. Use xvalidator for k-fold cross validation of a training dataset.
    2. Use classifier to construct a model from the training set and use the model file on the test set. The model file can be saved and applied to label subsequent test sets.

To run xvalidator:


   xvalidator -h  # (to see the help options)


To run classify:

   classifier -h  # (to see the help options)

The following sections provide detailed descriptions of these modules.


xvalidator:
-----------
The xvalidator uses k fold cross-validation to assess the prediction accuracy of an algorithm on a dataset. The user specifies the number of folds (k) and the xvalidator randomly divides the dataset into k different disjoint training and test folds. For each fold, a model is built based on the training data and applied on the test data to compute the accuracy of the built model on unseen data.

Positional arguments:

  settings_file  Settings file for configuring the machine learning algorithm
  ds_file        Dataset file
  num_folds      Number of folds

Optional arguments:

  -h, --help     show this help message and exit
  -d, --debug    Debug mode


The format and composition of the settings_file is defined in a subsequent section.

For using the libSVM package, a sample settings file is provided at conf/libsvm_settings.json. It is recommended to use this settings file without any modification for Twitter demographic inference tasks.

The ds_file needs to be in json format (generated by the tfx program of the twitter-feature-extractor). A sample file (sample_datafile.json) is provided at the samples/ folder.


Output:
  The first 3 lines prints the name of the ds_file separated by a sequence of asterisks.
  The next k lines report the computed accuracy of each folds.
  The next k lines present the predicted labels of the items in each fold as python dictionaries.
  The next line presents the average accuracy per fold.
  The last k lines presents aggregate information about label-wise accuracy for each folds.

Sample Command:
Run a 10-fold cross-validation on the given sample file: (from the ml-classifier project root)

     xvalidator conf/libsvm_settings.json samples/sample_datafile.json 10


classifier:
-----------

The classifier is used to i) build (and save) a model using a machine learning algorithm and ii) use the model to classify test data.


positional arguments:
  settings_file         Settings file for configuring the machine learning
                        algorithm
  test_ds_file          Test Dataset file

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL_FILE, --model_file MODEL_FILE
                        Model file to load/save.
  -t TRAINING_FILE, --training_file TRAINING_FILE
                        Training Dataset file
  -o OUTPUT_FILE, --output_file OUTPUT_FILE
                        output file to save
  -d, --debug           Debug mode

The format and composition of the settings_file is defined in a subsequent section. For using the libSVM package, a sample settings file is provided at ml2/libsvm_settings.json. It is recommended to use this settings file without any modification for Twitter demographic inference tasks.

The test_ds_file needs to be in json format (generated by the tfx program of the twitter-feature-extractor). A sample file (sample_datafile.json) is provided.

The meaning of MODEL_FILE is dependent on the TRAINING_FILE:
  i) When no MODEL_FILE is specified, the TRAINING_FILE must be specified, otherwise InvalidModelFileToReadException is thrown.
  ii) When both TRAINING_FILE and MODEL_FILE are both specified, the program assumes that a model based on the TRAINING_FILE needs to be constructed and the *saved* in the file specified as MODEL_FILE in the folder saved_model_information/.
  iii) When only the MODEL_FILE is specified, but no training file is given, the program assumes that a previously saved model needs to be loaded from the MODEL_FILE in the folder saved_model_information/.

output:
  The first 2-5 lines (depending on the options) print the name of the test_ds_file and training_ds_file separated by a sequence of asterisks.
  The next line present the predicted labels of the items in the test file as a python dictionary.
  The next line reports the overall prediction accuracy.
  The last line reports the predicted composition of the labels. It is useful when the study is about predicting the composition of different classes within the dataset.

Sample Command:

To create a model file named model_1 using the sample_datafile.json as the training set: (from the ml-classifier project root)

    classifier -t samples/sample_datafile.json -m model_1 conf/libsvm_settings.json samples/sample_datafile.json

To use the created model file model_1 to classify the instances in the test set sample_datafile.json:

    classifier -m model_1 conf/libsvm_settings.json samples/sample_datafile.json

As the training and test sets in the example are identical, the resulting accuracy is 100%.

Settings File Format:
---------------------
The settings files need to be in json/txt format encoding one single dictionary consisting 2 parameters. The trainer_class parameter specifies the python class name of the trainer. The trainer_settings spefies the algorithm specific parameters.


For the libsvm package, the following trainer_settings parameters are used:

  trainer_package_parameters: A single dictionary listing the libSVM parameters to be used by the trainer. However, if grid search is performed, the cost and gamma parameters is overriden by the value obtained by the grid search.

    -s type : set type of solver (default 1)
            0 -- L2-regularized logistic regression (primal)
            1 -- L2-regularized L2-loss support vector classification (dual)
            2 -- L2-regularized L2-loss support vector classification (primal)
            3 -- L2-regularized L1-loss support vector classification (dual)
            4 -- multi-class support vector classification by Crammer and Singer
            5 -- L1-regularized L2-loss support vector classification
            6 -- L1-regularized logistic regression
            7 -- L2-regularized logistic regression (dual)
    -c cost : set the parameter C (default 1)
    -g: gamma parameter.
    -e epsilon : set tolerance of termination criterion
            -s 0 and 2
                    |f'(w)|_2 <= eps*min(pos,neg)/l*|f'(w0)|_2,
                    where f is the primal function and pos/neg are # of
                    positive/negative data (default 0.01)
            -s 1, 3, 4 and 7
                    Dual maximal violation <= eps; similar to libsvm (default 0.1)
            -s 5 and 6
                    |f'(w)|_inf <= eps*min(pos,neg)/l*|f'(w0)|_inf,
                    where f is the primal function (default 0.01)
    -B bias : if bias >= 0, instance x becomes [x; bias]; if < 0, no bias term added (default -1)
    -wi weight: weights adjust the parameter C of different classes

  perform_grid_search: if true, a grid search to find optimal c and g is performed using the grid_search_parameters.
  grid_search_parameters:
    num_folds: Number of folds for cross-validation during the grid search.
    c_min,c_max: range for c
    g_min,g_max: range for g
    exp: if true, the c_min, c_max, g_min and g_max values provided are exponentiated. Otherwise, the exact values are used. Use exp for a greater exploration of the landscape.
    finer_search: whether to perform a finer search of the parameters. It has been found that it is prone to overfitting and not recommended.

For the GBM package only two parameters shrinkage and n_trees are used. Please refer to the documentation of gbm package for details.
